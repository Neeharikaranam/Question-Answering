{"cells":[{"cell_type":"markdown","metadata":{"id":"T0KQKMJ2tb6k"},"source":["# SQuAD Q\u0026A"]},{"cell_type":"markdown","metadata":{"id":"AVS64fpltb6m"},"source":["This notebook contains training scripts for models to be used for the question answering problem on the [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/) v1.1 dataset, which consists on selecting a possible answer to the given question as a span of words in the given context paragraph. The newest version (v2.0) of the dataset also contains unanswerable questions, but the one on which we worked on (v1.1) does not."]},{"cell_type":"markdown","metadata":{"id":"OTUJ-sN3tb6m"},"source":["## Colab requirements"]},{"cell_type":"markdown","metadata":{"id":"4wIlWiwQtb6m"},"source":["Before restarting runtime (remember to select GPU runtime)$\\dots$"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"wtVsT3sJtb6m"},"outputs":[{"name":"stdout","output_type":"stream","text":["Cloning into 'squad-question-answering'...\n","remote: Enumerating objects: 459, done.\u001b[K\n","remote: Total 459 (delta 0), reused 0 (delta 0), pack-reused 459\u001b[K\n","Receiving objects: 100% (459/459), 24.36 MiB | 19.11 MiB/s, done.\n","Resolving deltas: 100% (270/270), done.\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting numpy==1.19.2\n","  Downloading numpy-1.19.2.zip (7.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting pandas==1.1.3\n","  Downloading pandas-1.1.3-cp39-cp39-manylinux1_x86_64.whl (9.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m88.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting matplotlib==3.3.2\n","  Downloading matplotlib-3.3.2.tar.gz (37.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.9/37.9 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting scipy==1.5.3\n","  Downloading scipy-1.5.3.tar.gz (25.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.2/25.2 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting nltk==3.5\n","  Downloading nltk-3.5.zip (1.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting gensim==3.8.3\n","  Downloading gensim-3.8.3.tar.gz (23.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.4/23.4 MB\u001b[0m \u001b[31m71.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.7.0 (from versions: 1.7.1, 1.8.0, 1.8.1, 1.9.0, 1.9.1, 1.10.0, 1.10.1, 1.10.2, 1.11.0, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 2.0.0)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for torch==1.7.0\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["!git clone https://github.com/Wadaboa/squad-question-answering.git\n","!pip install -r squad-question-answering/init/base_requirements.txt"]},{"cell_type":"markdown","metadata":{"id":"dsTaK7khtb6n"},"source":["After restarting runtime$\\dots$"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"r8m9sTtTtb6n"},"outputs":[],"source":["import os, sys\n","\n","sys.path.insert(0, \"squad-question-answering\")\n","os.chdir(\"squad-question-answering\")"]},{"cell_type":"markdown","metadata":{"id":"A5-xsKnwtb6o"},"source":["## Imports"]},{"cell_type":"markdown","metadata":{"id":"xA2Ft8EQtb6o"},"source":["In order to import source files, we have to add the `src` folder to the Python `PATH`$\\dots$ "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"6q2tmKkEtb6o"},"outputs":[],"source":["import sys\n","\n","sys.path.insert(0, \"src\")"]},{"cell_type":"markdown","metadata":{"id":"Ck6aDuhStb6o"},"source":["Then, we can import packages as usual$\\dots$"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Iyy-7IaGtb6p"},"outputs":[],"source":["import os\n","from functools import partial\n","\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import wandb\n","import transformers\n","from transformers.trainer_utils import set_seed\n","\n","import dataset\n","import model\n","import training\n","import tokenizer\n","import utils\n","import layer_utils\n","import config\n","\n","%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"markdown","metadata":{"id":"lUNuUBjPtb6p"},"source":["Check the current configuration variables$\\dots$"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"gHR5kSqvtb6p","outputId":"299aea8a-6d2c-4fec-dda3-8cd86975e2e8"},"outputs":[{"data":{"text/plain":["[('BERT_VOCAB_PATH', 'data/bert-base-uncased-vocab.txt'),\n"," ('DATA_SUBSET', 1.0),\n"," ('EMBEDDING_DIMENSION', 200),\n"," ('EMBEDDING_MODEL_NAME', 'glove-twitter'),\n"," ('MAX_BERT_TOKENS', 512),\n"," ('MAX_CONTEXT_TOKENS', 300),\n"," ('PAD_TOKEN', '[PAD]'),\n"," ('RANDOM_SEED', 42),\n"," ('UNK_TOKEN', '[UNK]'),\n"," ('VAL_SPLIT', 0.2)]"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["[(item, getattr(config, item)) for item in dir(config) if not item.startswith(\"__\")]"]},{"cell_type":"markdown","metadata":{"id":"z_7i6H1stb6p"},"source":["## Initialization"]},{"cell_type":"markdown","metadata":{"id":"_S-Fn01Ltb6q"},"source":["In this section we are going to perform some initialization stuff for all the libraries to be used throughout the notebook$\\dots$"]},{"cell_type":"markdown","metadata":{"id":"3ul_XOdftb6q"},"source":["### Weights \u0026 biases"]},{"cell_type":"markdown","metadata":{"id":"ckB-Szh5tb6q"},"source":["Training and evaluation metrics, along with model checkpoints and results, are directly logged into a [W\u0026B](https://wandb.ai/) project, which is openly accessible [here](https://wandb.ai/wadaboa/squad-qa). Logging abilities are only granted to members of the team, so that if you want to launch your training run, you would have to disable wandb, by setting the environment variable `WANDB_DISABLED` to an empty value in the following block (`%env WANDB_DISABLED=`)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"RsevdVLwtb6q","outputId":"53a7148f-200f-44c2-898f-faea4394d7cd"},"outputs":[{"name":"stdout","output_type":"stream","text":["env: WANDB_PROJECT=squad-qa\n","env: WANDB_ENTITY=wadaboa\n","env: WANDB_MODE=online\n","env: WANDB_RESUME=never\n","env: WANDB_WATCH=false\n","env: WANDB_SILENT=true\n"]}],"source":["%env WANDB_PROJECT=squad-qa\n","%env WANDB_ENTITY=wadaboa\n","%env WANDB_MODE=online\n","%env WANDB_RESUME=never\n","%env WANDB_WATCH=false\n","%env WANDB_SILENT=true"]},{"cell_type":"markdown","metadata":{"id":"WASEsbaPtb6q"},"source":["Be sure to be logged in: if the system prompts you to insert a key, head over to [W\u0026B](https://wandb.ai/authorize), login and the key should appear on the web page$\\dots$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g88vNgXWtb6q"},"outputs":[],"source":["!wandb login"]},{"cell_type":"markdown","metadata":{"id":"Klr2OmDGtb6q"},"source":["Be sure to have `wandb` enabled system-wise$\\dots$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZkJM9t-Ptb6q"},"outputs":[],"source":["!wandb enabled"]},{"cell_type":"markdown","metadata":{"id":"NPAbTQHutb6r"},"source":["### PyTorch and numpy"]},{"cell_type":"markdown","metadata":{"id":"ZaEAnjT2tb6r"},"source":["Set the random seed to a fixed number for reproducible results$\\dots$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s8FwFSEatb6r"},"outputs":[],"source":["set_seed(config.RANDOM_SEED)"]},{"cell_type":"markdown","metadata":{"id":"cypElvUOtb6r"},"source":["Get the fastest device (GPU if available, else CPU as a fallback) to be used for training neural models in `PyTorch`$\\dots$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PEzh5QpCtb6r"},"outputs":[],"source":["DEVICE = utils.get_device()\n","DEVICE"]},{"cell_type":"markdown","metadata":{"id":"1dEQ22tptb6r"},"source":["If a GPU device is available, print related info like GPU type, current usage$\\dots$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JHSjII9Ttb6r"},"outputs":[],"source":["if DEVICE.type != \"cpu\":\n","    !nvidia-smi"]},{"cell_type":"markdown","metadata":{"id":"1FS5q9D9tb6r"},"source":["## Preliminaries"]},{"cell_type":"markdown","metadata":{"id":"KwZBNjuYtb6r"},"source":["In this section we are going to perform some preliminary steps, like data loading and common variables definition$\\dots$"]},{"cell_type":"markdown","metadata":{"id":"Onb4iC_Ttb6r"},"source":["### Raw data loading"]},{"cell_type":"markdown","metadata":{"id":"Gxvpc5l2tb6r"},"source":["The `SquadDataset` class holds a \"raw\" copy of the training set and the test set (if given). By \"raw\", we simply mean that questions and contexts are not pre-processed in this stage, but they are simply loaded from the given `JSON` files into appropriate `Pandas` `DataFrame`s.\n","\n","For the training set we used the official `SQuAD` v1.1 one, while for the test set we opted for the `SQuAD` v1.1 dev set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8_1HG-hrtb6r"},"outputs":[],"source":["DATA_FOLDER = os.path.join(os.getcwd(), \"data\")\n","TRAIN_DATA_FOLDER = os.path.join(DATA_FOLDER, \"training\")\n","TRAIN_SET_PATH = os.path.join(TRAIN_DATA_FOLDER, \"training_set.json\")\n","TEST_DATA_FOLDER = os.path.join(DATA_FOLDER, \"testing\")\n","TEST_SET_PATH = os.path.join(TEST_DATA_FOLDER, \"test_set.json\")"]},{"cell_type":"markdown","metadata":{"id":"GWQUEE4ytb6r"},"source":["Remember that the `subset` variable is used to load a random subset of both the training and testing dataset. This is to be used only for debugging purposes, so that `subset` should be set to $1.0$ when performing real training runs."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AYqqDhVBtb6s"},"outputs":[],"source":["squad_dataset = dataset.SquadDataset(\n","    train_set_path=TRAIN_SET_PATH,\n","    test_set_path=TEST_SET_PATH,\n","    subset=config.DATA_SUBSET,\n",")"]},{"cell_type":"markdown","metadata":{"id":"-VLYNpcttb6s"},"source":["Let's visualize the \"raw\" training set$\\dots$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZLQMimFotb6s"},"outputs":[],"source":["squad_dataset.raw_train_df"]},{"cell_type":"markdown","metadata":{"id":"CaFTPOTztb6s"},"source":["Let's visualize the \"raw\" test set$\\dots$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mC3icR7ktb6s"},"outputs":[],"source":["squad_dataset.raw_test_df"]},{"cell_type":"markdown","metadata":{"id":"rZ2Qk16atb6s"},"source":["### Utils"]},{"cell_type":"markdown","metadata":{"id":"3nPmlbhVtb6s"},"source":["This section contains common variables and functions to be used when training all the subsequent models$\\dots$"]},{"cell_type":"markdown","metadata":{"id":"eLeAo3mEtb6s"},"source":["The following cell is tasked to load the default training parameters, such as the batch size, the logging frequency, where model checkpoints should be saved and more$\\dots$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"olZsaZpNtb6s"},"outputs":[],"source":["TRAINER_ARGS = utils.get_default_trainer_args()"]},{"cell_type":"markdown","metadata":{"id":"Jge4AhJgtb6s"},"source":["## Recurrent models"]},{"cell_type":"markdown","metadata":{"id":"VE75ZyUTtb6s"},"source":["This section contains blocks of code that are used to train question answering models which are based on recurrent networks (`LSTM`s in our case). The models that we implemented are the following:\n","- Baseline (a recurrent encoder with a naive version of attention)\n","- BiDAF (Bi-Directional Attention Flow)\n","\n","Check the corresponding section to have a high-level view of each model$\\dots$"]},{"cell_type":"markdown","metadata":{"id":"ovAQ5_Rrtb6s"},"source":["### Embeddings"]},{"cell_type":"markdown","metadata":{"id":"1FsEiYA_tb6s"},"source":["In this section we are going to load an embedding matrix using the `Gensim` API and use the corresponding matrix as the weight block of an `nn.Embedding` `PyTorch` module$\\dots$"]},{"cell_type":"markdown","metadata":{"id":"qwL-7V3mtb6s"},"source":["First of all, we have to define one token for padding values. Then, OOV words are handled by a single unknown token, which is estimated as the mean of all the embedding vectors (if this mean vector is already present in the model, then a random embedding with suitable ranges is computed)."]},{"cell_type":"markdown","metadata":{"id":"9pufWEzBtb6s"},"source":["List of available embedding models (see [here](https://github.com/RaRe-Technologies/gensim-data)):\n","- FastText: \n","    - _fasttext-wiki-news-subwords_ (dimensions: 300)\n","- GloVe:\n","    - _glove-twitter_ (dimensions: 25. 50, 100, 200)\n","    - _glove-wiki-gigaword_ (dimensions: 50, 100, 200, 300)\n","- Word2Vec:\n","    - _word2vec-google-news_ (dimensions: 300)\n","    - _word2vec-ruscorpora_ (dimensions: 300)"]},{"cell_type":"markdown","metadata":{"id":"sQIcIM3Utb6s"},"source":["**Note**: The following cell could take a while (depending on the embedding dimension), since embedding models are pretty large$\\dots$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vATDbIgitb6s"},"outputs":[],"source":["embedding_model, vocab = utils.load_embedding_model(\n","    config.EMBEDDING_MODEL_NAME,\n","    embedding_dimension=config.EMBEDDING_DIMENSION,\n","    unk_token=config.UNK_TOKEN,\n","    pad_token=config.PAD_TOKEN,\n",")"]},{"cell_type":"markdown","metadata":{"id":"uooQs25Otb6s"},"source":["The following cell is tasked to load the embedding model into a `PyTorch` `nn.Embedding` layer, with frozen weights."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8DSrzyBntb6s"},"outputs":[],"source":["embedding_layer = layer_utils.get_embedding_module(\n","    embedding_model, pad_id=vocab[config.PAD_TOKEN]\n",")"]},{"cell_type":"markdown","metadata":{"id":"cr4t6iastb6t"},"source":["### Data loading"]},{"cell_type":"markdown","metadata":{"id":"fqrjKLGqtb6t"},"source":["The `SquadDataManager` class acts as both a data collator (i.e. it brings together multiple examples in the dataset with the help of `PyTorch`'s `DataLoader`s) and a tokenizer. In particular, tokenization happens on the fly at the batch level, thus enabling us to perform dynamic padding (based on the longest sequence in a batch) and avoiding the pre-tokenization overhead."]},{"cell_type":"markdown","metadata":{"id":"J2e99FUftb6t"},"source":["The tokenizer that we are using for recurrent modules splits words by whitespaces and punctuations, removes accents and applies a lowercasing function to all the tokens. Moreover, questions are padded (not truncated), while contexts are truncated to a maximum number of tokens and padded."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6jrVhx8Htb6t"},"outputs":[],"source":["recurrent_tokenizer = tokenizer.get_recurrent_tokenizer(\n","    vocab,\n","    config.MAX_CONTEXT_TOKENS,\n","    config.UNK_TOKEN,\n","    config.PAD_TOKEN,\n","    device=DEVICE,\n",")"]},{"cell_type":"markdown","metadata":{"id":"k38sSA8qtb6t"},"source":["The `SquadDataManager` class also acts as a pre-processor, with the following steps:\n","- Removes rows that contain wrong answers (e.g. answers that do not start and end at word boundaries)\n","- Removes rows that contain answers that would be lost due to tokenization (truncation in particular)\n","- Groups answers to the same question and context pair into a single row (thus producing lists in the `answer`, `answer_start` and `answer_end` columns)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kmfLCEb4tb6t"},"outputs":[],"source":["recurrent_dm = dataset.SquadDataManager(\n","    squad_dataset, recurrent_tokenizer, val_split=config.VAL_SPLIT, device=DEVICE\n",")"]},{"cell_type":"markdown","metadata":{"id":"x9j6lEF3tb6t"},"source":["The last task assigned to the `SquadDataManager` class is that of train/validation splitting, with the given ratio ($80\\%$ for the training set by default)."]},{"cell_type":"markdown","metadata":{"id":"to4KWjO0tb6t"},"source":["Let's have a look at the final training dataset$\\dots$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hterfziwtb6t"},"outputs":[],"source":["recurrent_dm.train_df"]},{"cell_type":"markdown","metadata":{"id":"ICvzlLxGtb6t"},"source":["Let's have a look at the final validation dataset$\\dots$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MM74pFZUtb6t"},"outputs":[],"source":["recurrent_dm.val_df"]},{"cell_type":"markdown","metadata":{"id":"9dbxD5p4tb6t"},"source":["And let's do the same for the testing dataset$\\dots$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ef8_Wn-0tb6t"},"outputs":[],"source":["recurrent_dm.test_df"]},{"cell_type":"markdown","metadata":{"id":"reYyZYdHtb6t"},"source":["### Baseline model"]},{"cell_type":"markdown","metadata":{"id":"UUVyefQmtb6t"},"source":["The baseline model is composed by a single recurrent encoder, which is given both questions and contexts as two separate inputs. Then, all the hidden states of a single question are averaged together (over the embedding dimension) so as to obtain a single vector which should encode the semantic information of the question at the sentence level. This aggregated question vector is then element-wise multiplied to each context token latent representation, so as to perform some kind of query-aware context encoding. Finally, the query-aware context vectors are passed onto another recurrent module and used as inputs for the end token classifier, while the query-aware context vectors are directly used as input for the start token classifier."]},{"cell_type":"markdown","metadata":{"id":"mc5i8aoItb6t"},"source":["Hyperparameters are carefully chosen by hand, considering the fact that the baseline model is pretty lightweight (in terms of FLOPS and parameters), so that we can afford using higher batch sizes. Moreover, the number of training epoch is set to a high value to understand if and when overfitting is observed."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w9fuWohPtb6u"},"outputs":[],"source":["%env WANDB_RUN_GROUP=baseline\n","baseline_run_name = utils.get_run_name()\n","baseline_args = partial(\n","    TRAINER_ARGS,\n","    output_dir=f\"./checkpoints/{os.getenv('WANDB_RUN_GROUP')}/{baseline_run_name}\",\n","    num_train_epochs=30,\n","    per_device_train_batch_size=128,\n","    per_device_eval_batch_size=128,\n",")"]},{"cell_type":"markdown","metadata":{"id":"2av96ahCtb6u"},"source":["#### Training and validation"]},{"cell_type":"markdown","metadata":{"id":"WPFCZRMytb6u"},"source":["In this section we will use the splitted train dataset to perform training and the splitted validation dataset to observe the evolution of metrics during training, at the end of each epoch. To be clear, the validation set is not used to tune hyperparameters, but just as a reference to report results over unseen data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WbMuRQERtb6u"},"outputs":[],"source":["baseline_model = model.QABaselineModel(embedding_layer, device=DEVICE)\n","print(f\"The baseline model has {baseline_model.count_parameters()} parameters\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OsZUGAt-tb6u"},"outputs":[],"source":["baseline_optimizer = optim.Adam(baseline_model.parameters(), lr=1e-3)\n","baseline_lr_scheduler = transformers.get_constant_schedule(baseline_optimizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1e9DEJzbtb6u"},"outputs":[],"source":["baseline_trainer = training.SquadTrainer(\n","    model=baseline_model,\n","    args=baseline_args(run_name=baseline_run_name),\n","    data_collator=recurrent_dm.tokenizer,\n","    train_dataset=recurrent_dm.train_dataset,\n","    eval_dataset=recurrent_dm.val_dataset,\n","    optimizers=(baseline_optimizer, baseline_lr_scheduler),\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5KrPqjLrzYqV"},"outputs":[],"source":["import wandb\n","\n","wandb.init(project=\"QA\", entity=\"nkaranam\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B-N-hvqltb6u"},"outputs":[],"source":["baseline_trainer.train()"]},{"cell_type":"markdown","metadata":{"id":"SVDeETHEtb6u"},"source":["#### Training only"]},{"cell_type":"markdown","metadata":{"id":"58wwOSCItb6u"},"source":["In this section we are going to use the whole dataset for training, so only the training loss will be used as a metric. This step is used to boost generalization ability and to exploit all the training data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-9_MRbt8tb6u"},"outputs":[],"source":["baseline_model = model.QABaselineModel(embedding_layer, device=DEVICE)\n","print(f\"The baseline model has {baseline_model.count_parameters()} parameters\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VtAxWqNltb6u"},"outputs":[],"source":["baseline_optimizer = optim.Adam(baseline_model.parameters(), lr=1e-3)\n","baseline_lr_scheduler = transformers.get_constant_schedule(baseline_optimizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"okVTFrkXtb6u"},"outputs":[],"source":["baseline_trainer = training.SquadTrainer(\n","    model=baseline_model,\n","    args=baseline_args(run_name=f\"{baseline_run_name}-whole\", evaluation_strategy=\"no\"),\n","    data_collator=recurrent_dm.tokenizer,\n","    train_dataset=recurrent_dm.whole_dataset,\n","    optimizers=(baseline_optimizer, baseline_lr_scheduler),\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HstlG3p5tb6u"},"outputs":[],"source":["baseline_trainer.train()"]},{"cell_type":"markdown","metadata":{"id":"7vU_cJxptb6u"},"source":["#### Testing"]},{"cell_type":"markdown","metadata":{"id":"rITtEjL0tb6u"},"source":["In this section we will use the test set to assess the generalization abilities of our model and observe final metrics$\\dots$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2rlzINa6tb6u"},"outputs":[],"source":["baseline_test_output = baseline_trainer.predict(recurrent_dm.test_dataset)\n","baseline_test_output.metrics"]},{"cell_type":"markdown","metadata":{"id":"hDSVk12Xtb6u"},"source":["And we are going to save a `JSON` file with the following schema:\n","```json\n","{\n","    \"question_id\": \"textual answer\"\n","    ...\n","}\n","```\n","The `JSON` files contains textual answers to each question in the given test dataset. The output file can also be used with the `SQuAD` official evaluation script$\\dots$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZfZZWwWHtb6u"},"outputs":[],"source":["baseline_answers_path = \"results/answers/baseline.json\"\n","utils.save_answers(baseline_answers_path, baseline_test_output.predictions[-1])\n","wandb.save(baseline_answers_path);\n","wandb.finish()"]},{"cell_type":"markdown","metadata":{"id":"xZ2cJ4sYtb6u"},"source":["### BiDAF"]},{"cell_type":"markdown","metadata":{"id":"FOpy36J2tb6v"},"source":["The Bi-Directional Attention Flow (BIDAF) network is a hierarchical\n","multi-stage architecture for modeling the representations of the context paragraph at different levels\n","of granularity. BIDAF includes character-level, word-level, and contextual embeddings,\n","and uses bi-directional attention flow to obtain a query-aware context representation.\n","Our attention mechanism offers following improvements to the previously popular attention paradigms. \n","First, the attention layer is not used to summarize the context paragraph into a fixed-size vector. Instead, the\n","attention is computed for every time step, and the attended vector at each time step, along with the\n","representations from previous layers, is allowed to flow through to the subsequent modeling layer.\n","This reduces the information loss caused by early summarization. Second, we use a memory-less\n","attention mechanism. That is, while we iteratively compute attention through time, the attention at each time step is a function of only the query and the context paragraph at the current time step and does not directly depend on the attention at the previous time step.\n","\n","The abstract from the paper is the following:\n","\u003e Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test."]},{"cell_type":"markdown","metadata":{"id":"t2NpHSejtb6v"},"source":["The main difference of our BiDAF model w.r.t. the full model reported in the paper is that we decided to avoid using character embeddings, since ablation studies showed that it gives only marginal improvements$\\dots$"]},{"cell_type":"markdown","metadata":{"id":"iMlpfprhtb6v"},"source":["Hyperparameters are taken directly from the BiDAF paper:\n","- Epochs: $12$ (we also tried an higher number of epochs)\n","- Batch size: $60$\n","- Optimizer: Adadelta\n","- Learning rate: $0.5$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W1oWskTAtb6v"},"outputs":[],"source":["%env WANDB_RUN_GROUP=bidaf\n","bidaf_run_name = utils.get_run_name()\n","bidaf_args = partial(\n","    TRAINER_ARGS,\n","    output_dir=f\"./checkpoints/{os.getenv('WANDB_RUN_GROUP')}/{bidaf_run_name}\",\n","    num_train_epochs=18,\n","    per_device_train_batch_size=60,\n","    per_device_eval_batch_size=60,\n",")"]},{"cell_type":"markdown","metadata":{"id":"ktUXF4Wltb6v"},"source":["#### Training and validation"]},{"cell_type":"markdown","metadata":{"id":"0N_c1Bn5tb6v"},"source":["As with the baseline model, we are going to perform both training and validation (to observe the evolution of metrics over unseen data)$\\dots$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3K9WxHvutb6v"},"outputs":[],"source":["bidaf_model = model.QABiDAFModel(embedding_layer, device=DEVICE)\n","print(f\"The BiDAF model has {bidaf_model.count_parameters()} parameters\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-BKcGiKUtb6v"},"outputs":[],"source":["bidaf_optimizer = optim.Adadelta(bidaf_model.parameters(), lr=0.5)\n","bidaf_lr_scheduler = transformers.get_constant_schedule(bidaf_optimizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K0-u1pDDtb6v"},"outputs":[],"source":["bidaf_trainer = training.SquadTrainer(\n","    model=bidaf_model,\n","    args=bidaf_args(run_name=bidaf_run_name),\n","    data_collator=recurrent_dm.tokenizer,\n","    train_dataset=recurrent_dm.train_dataset,\n","    eval_dataset=recurrent_dm.val_dataset,\n","    optimizers=(bidaf_optimizer, bidaf_lr_scheduler),\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o32PTMFLtb6v"},"outputs":[],"source":["bidaf_trainer.train()"]},{"cell_type":"markdown","metadata":{"id":"uaQnzbQNtb6v"},"source":["#### Training only"]},{"cell_type":"markdown","metadata":{"id":"IwiL9bKQtb6v"},"source":["As with the baseline model, we will use the entire dataset to perform training (no validation)$\\dots$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K_36AyERtb6v"},"outputs":[],"source":["bidaf_model = model.QABiDAFModel(embedding_layer, device=DEVICE)\n","print(f\"The BiDAF model has {bidaf_model.count_parameters()} parameters\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8XSsGCritb6v"},"outputs":[],"source":["bidaf_optimizer = optim.Adadelta(bidaf_model.parameters(), lr=0.5)\n","bidaf_lr_scheduler = transformers.get_constant_schedule(bidaf_optimizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y06hggxHtb6v"},"outputs":[],"source":["bidaf_trainer = training.SquadTrainer(\n","    model=bidaf_model,\n","    args=bidaf_args(run_name=f\"{bidaf_run_name}-whole\", evaluation_strategy=\"no\"),\n","    data_collator=recurrent_dm.tokenizer,\n","    train_dataset=recurrent_dm.whole_dataset,\n","    optimizers=(bidaf_optimizer, bidaf_lr_scheduler),\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hlXfgvbetb6v"},"outputs":[],"source":["bidaf_trainer.train()"]},{"cell_type":"markdown","metadata":{"id":"3lp94hFQtb6v"},"source":["#### Testing"]},{"cell_type":"markdown","metadata":{"id":"QoGCQpFrtb6v"},"source":["As with the baseline model, we will predict answers for each question in the given test dataset and save them on a `JSON` file$\\dots$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ix-2DQv4tb6v"},"outputs":[],"source":["bidaf_test_output = bidaf_trainer.predict(recurrent_dm.test_dataset)\n","bidaf_test_output.metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TJVJnr8otb6w"},"outputs":[],"source":["bidaf_answers_path = \"results/answers/bidaf.json\"\n","utils.save_answers(bidaf_answers_path, bidaf_test_output.predictions[-1])\n","wandb.save(bidaf_answers_path);\n","wandb.finish()"]},{"cell_type":"markdown","metadata":{"id":"Cfh31ooctb6w"},"source":["## Transformer networks"]},{"cell_type":"markdown","metadata":{"id":"W_ap52lItb6w"},"source":["This section contains blocks of code that are used to train question answering models which are based on Transformer networks (`BERT`-related in our case). The models that we exploited are the following:\n","- BERT (Bidirectional Encoder Representations from Transformers)\n","- DistilBERT (a distilled version of BERT)\n","- ELECTRA (Efficiently Learning an Encoder that Classifies Token Replacements Accurately)\n","\n","Since pre-training such models is very expensive, we relied on pre-trained versions of them (where the pre-training tasks are different than question answering), publicly available through the [HuggingFace](https://huggingface.co/) [Transformers](https://huggingface.co/transformers/) library. Pre-trained models are wrapped into an ad-hoc `PyTorch` module, which attaches to them the output layer to be used for question answering.\n","\n","Check the corresponding section to have a high-level view of each model$\\dots$"]},{"cell_type":"markdown","metadata":{"id":"09mAA-5Otb6w"},"source":["### Data loading"]},{"cell_type":"markdown","metadata":{"id":"jjYW7mrktb6w"},"source":["The tokenizer that we are using for Transformer-based modules splits words using the `WordPiece` algorithm, removes accents and applies a lowercasing function to all the tokens, while also merging together questions and contexts as $[CLS] q_1 q_2 \\dots q_n [SEP] c_1 c_2 \\dots c_m [SEP]$ (it leverages the special tokens `[CLS]` and `[SEP]`). Moreover, the combined question/context sentence is truncated to a maximum number of tokens ($512$) and padded to the right."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A3FaC3S2tb6w"},"outputs":[],"source":["transformer_tokenizer = tokenizer.get_transformer_tokenizer(\n","    config.BERT_VOCAB_PATH, config.MAX_BERT_TOKENS, device=DEVICE\n",")"]},{"cell_type":"markdown","metadata":{"id":"YV9KFWIhtb6w"},"source":["As with the recurrent-related part of the notebook, the `SquadDataManager` class pre-processes inputs by throwing away \"dirty\" and \"lost\" answers and then groups answers related to the same question and context pair into a single row."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O6jwYkLytb6w"},"outputs":[],"source":["transformer_dm = dataset.SquadDataManager(\n","    squad_dataset, transformer_tokenizer, val_split=config.VAL_SPLIT, device=DEVICE\n",")"]},{"cell_type":"markdown","metadata":{"id":"q0RvPHz6tb6w"},"source":["Let's see the training split of the whole dataset$\\dots$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4UyedJjutb6w"},"outputs":[],"source":["transformer_dm.train_df"]},{"cell_type":"markdown","metadata":{"id":"0AI2U5OJtb6w"},"source":["Let's see the validation split of the whole dataset$\\dots$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3DBPb21atb6w"},"outputs":[],"source":["transformer_dm.val_df"]},{"cell_type":"markdown","metadata":{"id":"30RQaDvctb6w"},"source":["And the same for the test dataset$\\dots$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CUVEUD14tb6w"},"outputs":[],"source":["transformer_dm.test_df"]},{"cell_type":"markdown","metadata":{"id":"VFrR82uCtb6w"},"source":["### BERT"]},{"cell_type":"markdown","metadata":{"id":"BPWE2QaHtb6w"},"source":["The BERT model is a bidirectional transformer pretrained using a combination of masked language modeling objective and next sentence prediction on a large corpus comprising the Toronto Book Corpus and Wikipedia.\n","\n","The abstract from the paper is the following:\n","\n","\u003e We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications."]},{"cell_type":"markdown","metadata":{"id":"7f7A6jc3tb6w"},"source":["Hyperparameters are taken directly from the BERT paper (in the section related to fine-tuning for the `SQuAD` dataset):\n","- Epochs: $3$\n","- Batch size: $32$ (we went for smaller sizes because of resources limitations)\n","- Learning rate: $5e-5$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iBrTpCkDtb6w"},"outputs":[],"source":["%env WANDB_RUN_GROUP=bert\n","bert_run_name = utils.get_run_name()\n","bert_args = partial(\n","    TRAINER_ARGS,\n","    output_dir=f\"./checkpoints/{os.getenv('WANDB_RUN_GROUP')}/{bert_run_name}\",\n","    num_train_epochs=3,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=16,\n",")"]},{"cell_type":"markdown","metadata":{"id":"YhJ0LLwBtb6w"},"source":["#### Training and validation"]},{"cell_type":"markdown","metadata":{"id":"erPeLIKbtb6w"},"source":["As with recurrent-based models, we are going to perform training and one validation run after each epoch, to observe metrics$\\dots$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9HsVVfqVtb6w"},"outputs":[],"source":["bert_model = model.QABertModel(device=DEVICE)\n","print(f\"The BERT model has {bert_model.count_parameters()} parameters\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Si8Pe8Iltb6w"},"outputs":[],"source":["bert_optimizer = optim.Adam(bert_model.parameters(), lr=5e-5)\n","bert_lr_scheduler = transformers.get_constant_schedule(bert_optimizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1QfqvPqdtb6x"},"outputs":[],"source":["bert_trainer = training.SquadTrainer(\n","    model=bert_model,\n","    args=bert_args(run_name=bert_run_name),\n","    data_collator=transformer_dm.tokenizer,\n","    train_dataset=transformer_dm.train_dataset,\n","    eval_dataset=transformer_dm.val_dataset,\n","    optimizers=(bert_optimizer, bert_lr_scheduler),\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JykBAcdltb6x"},"outputs":[],"source":["bert_trainer.train()"]},{"cell_type":"markdown","metadata":{"id":"SIESjyyotb6x"},"source":["#### Training only"]},{"cell_type":"markdown","metadata":{"id":"yqxNnsm8tb6x"},"source":["As with recurrent-based modules, we are going to perform training over the whole dataset (with no validation at all)$\\dots$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"86EcguKttb6x"},"outputs":[],"source":["bert_model = model.QABertModel(device=DEVICE)\n","print(f\"The BERT model has {bert_model.count_parameters()} parameters\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1X80WUmztb6x"},"outputs":[],"source":["bert_optimizer = optim.Adam(bert_model.parameters(), lr=5e-5)\n","bert_lr_scheduler = transformers.get_constant_schedule(bert_optimizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RjhqceePtb6x"},"outputs":[],"source":["bert_trainer = training.SquadTrainer(\n","    model=bert_model,\n","    args=bert_args(run_name=f\"{bert_run_name}-whole\", evaluation_strategy=\"no\"),\n","    data_collator=transformer_dm.tokenizer,\n","    train_dataset=transformer_dm.whole_dataset,\n","    optimizers=(bert_optimizer, bert_lr_scheduler),\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QhPjrXiKtb6x"},"outputs":[],"source":["bert_trainer.train()"]},{"cell_type":"markdown","metadata":{"id":"UExnau-itb6x"},"source":["#### Testing"]},{"cell_type":"markdown","metadata":{"id":"n4IdJhpttb6x"},"source":["As with recurrent-based modules, we will predict one answer for each question in the test dataset and save results in a `JSON` file$\\dots$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MBmiftkEtb6x"},"outputs":[],"source":["bert_test_output = bert_trainer.predict(transformer_dm.test_dataset)\n","bert_test_output.metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8Dzj-or5tb6x"},"outputs":[],"source":["bert_answers_path = \"results/answers/bert.json\"\n","utils.save_answers(bert_answers_path, bert_test_output.predictions[-1])\n","wandb.save(bert_answers_path);\n","wandb.finish()"]},{"cell_type":"markdown","metadata":{"id":"wfyVPQ-ctb6x"},"source":["### DistilBERT"]},{"cell_type":"markdown","metadata":{"id":"U8bjOlt8tb6x"},"source":["DistilBERT is a small, fast, cheap and light Transformer model trained by distilling BERT base. It has 40% less parameters than _bert-base-uncased_, runs 60% faster while preserving over 95% of BERT’s performances as measured on the GLUE language understanding benchmark.\n","\n","The abstract from the paper is the following:\n","\n","\u003e As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pretraining phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pretraining, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study."]},{"cell_type":"markdown","metadata":{"id":"WlpWzgB-tb6x"},"source":["Hyperparameters are the same as the ones used for fine-tuning BERT$\\dots$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"umXAh1fqtb6x"},"outputs":[],"source":["%env WANDB_RUN_GROUP=distilbert\n","distilbert_run_name = utils.get_run_name()\n","distilbert_args = partial(\n","    TRAINER_ARGS,\n","    output_dir=f\"./checkpoints/{os.getenv('WANDB_RUN_GROUP')}/{distilbert_run_name}\",\n","    num_train_epochs=3,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=16,\n",")"]},{"cell_type":"markdown","metadata":{"id":"Vlmnninttb6x"},"source":["#### Training and validation"]},{"cell_type":"markdown","metadata":{"id":"G-aN6eKBtb6x"},"source":["Same reasoning as above$\\dots$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tGN4TE-Gtb6x"},"outputs":[],"source":["distilbert_model = model.QADistilBertModel(device=DEVICE)\n","print(f\"The DistilBERT model has {distilbert_model.count_parameters()} parameters\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_YO3zLMZtb6x"},"outputs":[],"source":["distilbert_optimizer = optim.Adam(distilbert_model.parameters(), lr=5e-5)\n","distilbert_lr_scheduler = transformers.get_constant_schedule(distilbert_optimizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LIPJ8RP-tb6y"},"outputs":[],"source":["distilbert_trainer = training.SquadTrainer(\n","    model=distilbert_model,\n","    args=distilbert_args(run_name=distilbert_run_name),\n","    data_collator=transformer_dm.tokenizer,\n","    train_dataset=transformer_dm.train_dataset,\n","    eval_dataset=transformer_dm.val_dataset,\n","    optimizers=(distilbert_optimizer, distilbert_lr_scheduler),\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Eu0O14Rmtb6y"},"outputs":[],"source":["distilbert_trainer.train()"]},{"cell_type":"markdown","metadata":{"id":"u30bhJSOtb6y"},"source":["#### Training only"]},{"cell_type":"markdown","metadata":{"id":"Z8y7G4hxtb6y"},"source":["Same reasoning as above$\\dots$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DCHr58njtb6y"},"outputs":[],"source":["distilbert_model = model.QADistilBertModel(device=DEVICE)\n","print(f\"The DistilBERT model has {distilbert_model.count_parameters()} parameters\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mu7R85Xytb6y"},"outputs":[],"source":["distilbert_optimizer = optim.Adam(distilbert_model.parameters(), lr=5e-5)\n","distilbert_lr_scheduler = transformers.get_constant_schedule(distilbert_optimizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QG6RQV2ltb6y"},"outputs":[],"source":["distilbert_trainer = training.SquadTrainer(\n","    model=distilbert_model,\n","    args=distilbert_args(run_name=f\"{distilbert_run_name}-whole\", evaluation_strategy=\"no\"),\n","    data_collator=transformer_dm.tokenizer,\n","    train_dataset=transformer_dm.whole_dataset,\n","    optimizers=(distilbert_optimizer, distilbert_lr_scheduler),\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KF6obWeGtb6y"},"outputs":[],"source":["distilbert_trainer.train()"]},{"cell_type":"markdown","metadata":{"id":"ZF7Ptjuntb6y"},"source":["#### Testing"]},{"cell_type":"markdown","metadata":{"id":"UET_O_Oytb6y"},"source":["Same reasoning as above$\\dots$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qBdJ928Otb6y"},"outputs":[],"source":["distilbert_test_output = distilbert_trainer.predict(transformer_dm.test_dataset)\n","distilbert_test_output.metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V5kfj6W0tb6y"},"outputs":[],"source":["distilbert_answers_path = \"results/answers/distilbert.json\"\n","utils.save_answers(distilbert_answers_path, distilbert_test_output.predictions[-1])\n","wandb.save(distilbert_answers_path);\n","wandb.finish()"]},{"cell_type":"markdown","metadata":{"id":"mYDEgdG9tb6y"},"source":["### ELECTRA"]},{"cell_type":"markdown","metadata":{"id":"iX2aIvxJtb6y"},"source":["ELECTRA is a new pretraining approach which trains two transformer models: the generator and the discriminator. The generator’s role is to replace tokens in a sequence, and is therefore trained as a masked language model. The discriminator, which is the model we’re interested in, tries to identify which tokens were replaced by the generator in the sequence.\n","\n","The abstract from the paper is the following:\n","\n","\u003e Masked language modeling (MLM) pretraining methods such as BERT corrupt the input by replacing some tokens with [MASK] and then train a model to reconstruct the original tokens. While they produce good results when transferred to downstream NLP tasks, they generally require large amounts of compute to be effective. As an alternative, we propose a more sample-efficient pretraining task called replaced token detection. Instead of masking the input, our approach corrupts it by replacing some tokens with plausible alternatives sampled from a small generator network. Then, instead of training a model that predicts the original identities of the corrupted tokens, we train a discriminative model that predicts whether each token in the corrupted input was replaced by a generator sample or not. Thorough experiments demonstrate this new pretraining task is more efficient than MLM because the task is defined over all input tokens rather than just the small subset that was masked out. As a result, the contextual representations learned by our approach substantially outperform the ones learned by BERT given the same model size, data, and compute. The gains are particularly strong for small models; for example, we train a model on one GPU for 4 days that outperforms GPT (trained using 30x more compute) on the GLUE natural language understanding benchmark. Our approach also works well at scale, where it performs comparably to RoBERTa and XLNet while using less than 1/4 of their compute and outperforms them when using the same amount of compute."]},{"cell_type":"markdown","metadata":{"id":"b4bpg-uftb6y"},"source":["Hyperparameters are the same as the ones used for fine-tuning BERT and DistilBERT$\\dots$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nJeoPlHUtb6y"},"outputs":[],"source":["%env WANDB_RUN_GROUP=electra\n","electra_run_name = utils.get_run_name()\n","electra_args = partial(\n","    TRAINER_ARGS,\n","    output_dir=f\"./checkpoints/{os.getenv('WANDB_RUN_GROUP')}/{electra_run_name}\",\n","    num_train_epochs=3,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=16,\n",")"]},{"cell_type":"markdown","metadata":{"id":"-ZnMOu96tb6y"},"source":["#### Training and validation"]},{"cell_type":"markdown","metadata":{"id":"IAfM2jtStb6y"},"source":["Same reasoning as above$\\dots$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"569X62_Ctb6y"},"outputs":[],"source":["electra_model = model.QAElectraModel(device=DEVICE)\n","print(f\"The ELECTRA model has {electra_model.count_parameters()} parameters\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"euU6yiEUtb6y"},"outputs":[],"source":["electra_optimizer = optim.Adam(electra_model.parameters(), lr=5e-5)\n","electra_lr_scheduler = transformers.get_constant_schedule(electra_optimizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iXUDt1Uttb6y"},"outputs":[],"source":["electra_trainer = training.SquadTrainer(\n","    model=electra_model,\n","    args=electra_args(run_name=electra_run_name),\n","    data_collator=transformer_dm.tokenizer,\n","    train_dataset=transformer_dm.train_dataset,\n","    eval_dataset=transformer_dm.val_dataset,\n","    optimizers=(electra_optimizer, electra_lr_scheduler),\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2xyqws8Btb6z"},"outputs":[],"source":["electra_trainer.train()"]},{"cell_type":"markdown","metadata":{"id":"WAh1hSc5tb6z"},"source":["#### Training only"]},{"cell_type":"markdown","metadata":{"id":"8B_VKG8ytb6z"},"source":["Same reasoning as above$\\dots$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZWgSD0putb6z"},"outputs":[],"source":["electra_model = model.QAElectraModel(device=DEVICE)\n","print(f\"The ELECTRA model has {electra_model.count_parameters()} parameters\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dl1py_cTtb6z"},"outputs":[],"source":["electra_optimizer = optim.Adam(electra_model.parameters(), lr=5e-5)\n","electra_lr_scheduler = transformers.get_constant_schedule(electra_optimizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hOLYifJEtb6z"},"outputs":[],"source":["electra_trainer = training.SquadTrainer(\n","    model=electra_model,\n","    args=electra_args(run_name=f\"{electra_run_name}-whole\", evaluation_strategy=\"no\"),\n","    data_collator=transformer_dm.tokenizer,\n","    train_dataset=transformer_dm.whole_dataset,\n","    optimizers=(electra_optimizer, electra_lr_scheduler),\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EuMe3aA0tb6z"},"outputs":[],"source":["electra_trainer.train()"]},{"cell_type":"markdown","metadata":{"id":"ftL4mTlTtb6z"},"source":["#### Testing"]},{"cell_type":"markdown","metadata":{"id":"ukhz6ofbtb6z"},"source":["Same reasoning as above$\\dots$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uy_WEhoxtb6z"},"outputs":[],"source":["electra_test_output = electra_trainer.predict(transformer_dm.test_dataset)\n","electra_test_output.metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7m1ZrXVltb6z"},"outputs":[],"source":["electra_answers_path = \"results/answers/electra.json\"\n","utils.save_answers(electra_answers_path, electra_test_output.predictions[-1])\n","wandb.save(electra_answers_path);\n","wandb.finish()"]}],"metadata":{"accelerator":"GPU","colab":{"name":"","version":""},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":0}